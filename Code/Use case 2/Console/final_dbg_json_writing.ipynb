{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "!pip install mxnet \n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "interval = 'D' #Use D or H\n",
    "\n",
    "assert interval == 'D' \n",
    "\n",
    "if interval == 'D':\n",
    "    prediction_length = 65 #when interval = D    \n",
    "    context_length = 65\n",
    "    \n",
    "mnemonics = ['CON','DAI','PAH3','BMW','VOW3']\n",
    "target_column = 'EndPrice'\n",
    "covariate_columns = ['StartPrice', 'MinPrice', 'MaxPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role and session\n",
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "#Obtain container image URI for SageMaker-DeepAR algorithm, based on region\n",
    "region = session.boto_region_name\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "print(\"Model will be trained using container image : {}\".format(image_name))\n",
    "\n",
    "#Define training data location\n",
    "s3_data_key = 'dbg-stockdata/source'\n",
    "s3_bucket = session.default_bucket()\n",
    "s3_output_path = \"s3://{}/{}/{}/output\".format(s3_bucket, s3_data_key, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role and session\n",
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "#Obtain container image URI for SageMaker-DeepAR algorithm, based on region\n",
    "region = session.boto_region_name\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "print(\"Model will be trained using container image : {}\".format(image_name))\n",
    "\n",
    "#Define training data location\n",
    "s3_data_key = 'dbg-stockdata/source'\n",
    "s3_bucket = session.default_bucket()\n",
    "s3_output_path = \"s3://{}/{}/{}/output\".format(s3_bucket, s3_data_key, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "selected = stock_data_series.sort_index()\n",
    "train_set, test_set= np.split(selected, [int(.8 *len(selected))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gluonts\n",
    "import datetime\n",
    "from gluonts.dataset.common import ListDataset\n",
    "# Function to convert data frames containing time series data to JSON serialized data that DeepAR works with\n",
    "def json_serialize(data, start, end, target_column, covariate_columns, interval):\n",
    "    timeseries = {}\n",
    "\n",
    "    for i, col in enumerate(data.columns):\n",
    "        metric = col[col.find('-')+1:]\n",
    "        stock = col[:col.find('-')]\n",
    "        if metric == target_column:\n",
    "            if stock in timeseries.keys():\n",
    "                timeseries[stock][\"target\"] = data.iloc[:,i][start:end]\n",
    "            else:\n",
    "                timeseries[stock] = {}\n",
    "                timeseries[stock][\"start\"] = str(pd.Timestamp(datetime.datetime.strptime(str(start), \"%Y-%m-%d %H:%M:%S\").strftime(\"%Y-%m-%d %H:%M:%S\"), freq = interval))\n",
    "                timeseries[stock][\"target\"] = data.iloc[:,i][start:end]            \n",
    "            print(\"Time series for {} added\".format(stock))\n",
    "        elif metric in covariate_columns:\n",
    "            if stock in timeseries.keys():\n",
    "                if \"dynamic_feat\" in timeseries[stock]:\n",
    "                    dynamic_feat = timeseries[stock][\"dynamic_feat\"]\n",
    "                    dynamic_feat.append(data.iloc[:,i][start:end])\n",
    "                else:\n",
    "                    dynamic_feat = []\n",
    "                    dynamic_feat.append(data.iloc[:,i][start:end])\n",
    "                    timeseries[stock][\"dynamic_feat\"] = dynamic_feat\n",
    "            else:\n",
    "                timeseries[stock] = {}\n",
    "                dynamic_feat = []\n",
    "                dynamic_feat.append(data.iloc[:,i])\n",
    "                timeseries[stock][\"dynamic_feat\"] = dynamic_feat            \n",
    "            print(\"Dynamic Feature - {} for {} added\".format(metric, stock))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    json_data = \n",
    "        {\n",
    "            \"start\": ts[\"start\"],\n",
    "            \"target\": ts[\"target\"].tolist(),  \n",
    "            \"dynamic_feat\": [feat.tolist() for feat in ts[\"dynamic_feat\"]]\n",
    "        }\n",
    "        for ts in timeseries.values()\n",
    "    \n",
    "    return json_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_dt = train_set.index[0]\n",
    "train_end_dt = train_set.index[-1]   \n",
    "test_start_dt = test_set.index[0]\n",
    "test_end_dt = test_set.index[-1]   \n",
    "\n",
    "train_json = json_serialize(train_set, train_start_dt, train_end_dt, target_column, covariate_columns, interval)\n",
    "test_json = json_serialize(test_set, test_start_dt, test_end_dt, target_column, covariate_columns, interval)\n",
    "with open('train.json', 'w') as train_file:\n",
    "    json.dump(train_json, train_file)\n",
    "with open('test.json', 'w') as test_file:\n",
    "    json.dump(test_json, test_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
